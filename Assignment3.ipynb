{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1ncupt2CxdbyYC6VgepY84jpiqZpV8UkR",
      "authorship_tag": "ABX9TyN1o49QjrJh37Jw4THxEpmV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RaRagi-Bliss/Python-Text-Analyzer/blob/imp-LING226-final-asgn/Assignment3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Necessary links and documents for this assignment\n",
        "\n",
        "text data\n",
        "https://drive.google.com/drive/folders/11IlS87sebzk6D02Ks61MNKbAOu6aKw08?usp=sharing\n",
        "\n",
        "report\n",
        "https://docs.google.com/document/d/1LTluiE6PhXJilCgR17c1JrCsZ9U9MhrdsQ585GhWR78/edit?usp=sreport\n",
        "\n",
        "spreadsheet data\n",
        "https://docs.google.com/spreadsheets/d/1DL2CJ3KbLecQjci2Kc6raacRLCMXVFRs4bEr8ugySBQ/edit?usp=sharing"
      ],
      "metadata": {
        "id": "xACBRPNIO-x1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "9jkTQwUx4nOR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#import\n",
        "import nltk # import the main nltk module\n",
        "import re # import regular expression\n",
        "import csv # import spreadsheets\n",
        "import os\n",
        "import warnings\n",
        "from nltk.corpus import stopwords #import stop words\n",
        "from nltk.corpus import cmudict\n",
        "from nltk.corpus import words\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.tokenize import SyllableTokenizer\n",
        "from nltk.probability import FreqDist\n",
        "from collections import Counter\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "\n",
        "nltk.download('punkt_tab')  # previously nltk.download('punkt'), but has since\n",
        "                            # been deprecated, replaced with ('punkt_tab')\n",
        "nltk.download('words')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('vader_lexicon')\n",
        "nltk.download('cmudict')\n",
        "\n",
        "sid = SentimentIntensityAnalyzer()\n",
        "cmu = cmudict.dict()"
      ],
      "metadata": {
        "id": "cbpTdPH5JjR7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy\n",
        "!python -m spacy download en_core_web_sm\n",
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')"
      ],
      "metadata": {
        "id": "vp6_AUT5YPk4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "yy-SkYQbQ5w_"
      },
      "outputs": [],
      "source": [
        "#Preprocessing\n",
        "def remove_punctuation(text):\n",
        "    if isinstance(text, str):  # Check if input is a string\n",
        "        return re.sub(r'[^\\w\\s]', '', text)  # Remove all punctuation except for underscores, words, and spaces\n",
        "    else:\n",
        "        raise ValueError(\"Input should be a string\")\n",
        "\n",
        "def remove_stopWords(text):\n",
        "  text_tokens = text #word_tokenize(text)\n",
        "  stoppedWords = [word for word in text_tokens if not word in stopwords.words('english')]\n",
        "  output = ''.join(text)\n",
        "  return output\n",
        "\n",
        "#broken\n",
        "def filterNonWords(text):\n",
        "    # Tokenize the lyrics into words\n",
        "    tokens = nltk.word_tokenize(text.lower())   # Convert to lowercase for consistency\n",
        "    english_words = set(words.words())            # Get a set of words from the NLTK words corpus\n",
        "    filtered_words = [word for word in tokens if word in english_words]     # Filter words that are in the English dictionary\n",
        "    filtered_lyrics = ' '.join(filtered_words)    # Join the filtered words back into a string\n",
        "    return filtered_lyrics\n",
        "\n",
        "#broken\n",
        "def removeShortWords(text):\n",
        "    tokens = word_tokenize(text) # Tokenize the text into words\n",
        "    filtered_tokens = [word for word in tokens if len(word) > 3]  # Filter out words that are three characters or shorter\n",
        "    filtered_text = ''.join(filtered_tokens)     # Join the remaining words back into a string\n",
        "\n",
        "    return filtered_text\n",
        "\n",
        "def preprocess(text):\n",
        "  input = text\n",
        "  one = remove_punctuation(input)\n",
        "  output = remove_stopWords(one)\n",
        "  #output = filterNonWords(two) broken\n",
        "  #output = removeShortWords(three) broken\n",
        "  return output\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#analysis\n",
        "\n",
        "def totalWords(text):\n",
        "  return len(nltk.word_tokenize(text))\n",
        "\n",
        "def totalUniqueWords(text):\n",
        "  return len(set(nltk.word_tokenize(text)))\n",
        "\n",
        "def countSyllables(text):\n",
        "  # Initialize a SyllableTokenizer instance\n",
        "  syllable_tokenizer = SyllableTokenizer()\n",
        "  # Tokenize the text into syllables\n",
        "  syllables = syllable_tokenizer.tokenize(text)\n",
        "  return len(syllables)\n",
        "\n",
        "def countSentences(text):\n",
        "  sentences = text.split('\\n')\n",
        "  return len(sentences)\n",
        "\n",
        "def lexicalDiversity(text):\n",
        "  print(f\"Total number of words: \\t{totalWords(text)}\")\n",
        "  print(f\"Total unique words: \\t{len(set(nltk.word_tokenize(text.lower())))}\")\n",
        "\n",
        "  ld = len(set(nltk.word_tokenize(text)))/len(nltk.word_tokenize(text))\n",
        "\n",
        "  print(f\"Total repeated words: \\t{totalWords(text) - len(set(nltk.word_tokenize(text.lower())))}\")\n",
        "  print(f\"Lexical Diversity of Text: {ld}\")\n",
        "\n",
        "  sentences = sent_tokenize(text)\n",
        "  total_sentence_ld = 0\n",
        "\n",
        "  for sentence in sentences:\n",
        "    sentence_words = word_tokenize(sentence.lower())\n",
        "    sentence_unique_words = set(sentence_words)\n",
        "\n",
        "    sentence_words_count = len(sentence_words)\n",
        "    sentence_unique_words_count = len(sentence_unique_words)\n",
        "\n",
        "    if sentence_words_count > 0:\n",
        "      sentence_ld = sentence_unique_words_count / sentence_words_count\n",
        "      total_sentence_ld += sentence_ld\n",
        "\n",
        "  if len(sentences) > 0:\n",
        "    average_sentence_ld = total_sentence_ld / len(sentences)\n",
        "    print(f\"Average Lexical Diversity per sentence: {average_sentence_ld}\")\n",
        "  else:\n",
        "    print(\"No sentences found in the text.\")\n",
        "\n",
        "def wordFrequencies(text):\n",
        "  input = text.lower()\n",
        "  text_tokens = input.split(\" \")\n",
        "  text_fdist = FreqDist(text_tokens)\n",
        "  print(f\"Most Common Words: {text_fdist.most_common(10)}\")\n",
        "\n",
        "def count_syllables_v2(word):\n",
        "\n",
        "    if word in cmu.keys():\n",
        "      # get the entry from the dictionary, and slice the first set of sounds\n",
        "      phones = cmu[word][0]\n",
        "\n",
        "      # using a list comprehension, extract only the sounds that end in a digit\n",
        "      vowel_sounds= [sound for sound in phones if sound[-1].isdigit()]\n",
        "\n",
        "      # the number of syllabes is equal to the number of vowel_sounds,\n",
        "      syllables = len(vowel_sounds)\n",
        "\n",
        "    else:\n",
        "      # If the word is not found in the dictionary, use a simple rule\n",
        "      # based on the number of vowel letters\n",
        "      vowels = \"aeiouy\"\n",
        "      syllables = 0\n",
        "      prev_char_is_vowel = False\n",
        "      for char in word:\n",
        "          if char in vowels:\n",
        "              if not prev_char_is_vowel:\n",
        "                  syllables += 1\n",
        "              prev_char_is_vowel = True\n",
        "          else:\n",
        "              prev_char_is_vowel = False\n",
        "\n",
        "    #print(f'The word {word} has {syllables} syllables.')\n",
        "    return syllables\n",
        "\n",
        "def text_info(text):\n",
        "  \"\"\"\n",
        "  Args:\n",
        "    text: a string\n",
        "  Returns:\n",
        "    ...\n",
        "  \"\"\"\n",
        "  # lowercase the text\n",
        "  text = text.lower()\n",
        "\n",
        "  # extract tokens, removing any that are just punctuation\n",
        "  tokens = [token.lower() for token in nltk.word_tokenize(text) if token.isalpha()]\n",
        "\n",
        "  # extract sentences\n",
        "  sentences = countSentences(text)\n",
        "\n",
        "  # extract syllables\n",
        "  syllables = 0\n",
        "\n",
        "  for token in tokens:\n",
        "    syllables += count_syllables_v2(token)\n",
        "  print(f'This text has {len(tokens)} words, {sentences} sentences, \\nand {syllables} syllables.')\n",
        "  return len(tokens), sentences, syllables\n",
        "\n",
        "def fleschReadingEase(text):\n",
        "  words, sents, sylls = text_info(text)\n",
        "\n",
        "  word_sents = words/sents\n",
        "  syll_words = sylls/words\n",
        "\n",
        "  reading_ease_score = 206.835 - (1.015 * word_sents) - (84.6 * syll_words)\n",
        "\n",
        "  print(f'Flesch Reading Ease Score: {reading_ease_score}')\n",
        "\n",
        "def sentiment_lookup(text):\n",
        "  #calculates average sentiment\n",
        "  tokens = nltk.word_tokenize(text.lower())\n",
        "  output = []\n",
        "\n",
        "  for token in tokens:\n",
        "    # first check if the token is even in the dictionary\n",
        "    if token in sid.lexicon.keys():\n",
        "      # add the rating to the output\n",
        "      output.append(sid.lexicon[token])\n",
        "\n",
        " # we calculate the average sentiment of the text from all the values in our output\n",
        "  if output:\n",
        "    avg_sentiment = sum(output)/len(output)\n",
        "  # how many words from the sentence were actually used in the calculation\n",
        "    print(f'Sentiment is: {avg_sentiment},\\nusing {round(len(output)/len(tokens)*100, 2)}% of the words in the text,\\nwhich is {len(output)} words total')\n",
        "  else:\n",
        "    print('Nothing in this text was in the sentiment dictionary')\n",
        "\n",
        "##Implement saving to file(Book 17)\n",
        "\n"
      ],
      "metadata": {
        "id": "i0K5Ji_nJulb"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Showing Data\n",
        "def textInformation(text):\n",
        "\n",
        "  with warnings.catch_warnings():\n",
        "    warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"nltk\")\n",
        "    print (\"---------------------------------\")\n",
        "    print (\"\\t TEXT INFORMATION \\n\")\n",
        "    print (f\"Total number of words: \\t{totalWords(text)}\")\n",
        "    print (f\"Total unique words: \\t{totalUniqueWords(text)}\")\n",
        "    print (f\"Number of Syllables:  \\t{countSyllables(text)}\")\n",
        "    print (f\"Number of Sentences:  \\t{countSentences(text)}\")\n",
        "    print (\"\\n---------------------------------\")\n",
        "    print (\"\\t TEXT SENTIMENT \\n\")\n",
        "    sentiment_lookup(text)\n",
        "    print (\"\\n---------------------------------\")\n",
        "    print (\"\\t FLESCH READING \\n\")\n",
        "    fleschReadingEase(text)\n",
        "    print (\"\\n---------------------------------\")\n",
        "    print (\"\\t LEXICAL DIVERSITY \\n\")\n",
        "    lexicalDiversity(text)\n",
        "    print (\"\\n---------------------------------\")\n",
        "    print (\"\\t TOP 10 WORDS\")\n",
        "    wordFrequencies(text)\n",
        "    print (\"\\n---------------------------------\")\n"
      ],
      "metadata": {
        "id": "Sjg-hzSHJwpK"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Art Museum\n",
        "folder_path = \"/content/drive/MyDrive/LING226/Art Museum (Musee d'Orsay)\""
      ],
      "metadata": {
        "id": "8EkLGgrmkagL"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#History Museum\n",
        "folder_path = \"/content/drive/MyDrive/LING226/History Museum (Abbey Library of Saint Gall)\""
      ],
      "metadata": {
        "id": "Eiv8GRfpkm9r"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#National Museum\n",
        "folder_path = \"/content/drive/MyDrive/LING226/National Museum (Zurich Museum)\""
      ],
      "metadata": {
        "id": "zIUMm8viIyae"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#National Museum\n",
        "folder_path = \"/content/drive/MyDrive/LING226/National Museum (Te Papa)\""
      ],
      "metadata": {
        "id": "GtaDhcGE51gj"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# List files in the folder\n",
        "file_list = os.listdir(folder_path)\n",
        "\n",
        "# Display the list of files\n",
        "print(\"Files in 'text_files' folder:\")\n",
        "for file_name in file_list:\n",
        "    print(file_name)\n",
        "\n",
        "for file_name in file_list:\n",
        "    full_path = os.path.join(folder_path, file_name)\n",
        "    if file_name.endswith('.txt'):  # Check if it's a text file\n",
        "        with open(full_path, 'r') as file:\n",
        "            content = file.read()\n",
        "            print(f\"Contents on {file_name}:\")\n",
        "            textInformation(preprocess(content))\n",
        "            doc = nlp(content)\n",
        "            noun_phrases = [chunk.text for chunk in doc.noun_chunks]\n",
        "            print(f'Noun Phrases in {file_name}: \\n {noun_phrases}')"
      ],
      "metadata": {
        "id": "Iur5_rZOkRch",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}